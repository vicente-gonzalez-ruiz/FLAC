{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [FLAC (Free Lossless Audio Codec)](https://xiph.org/flac/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lossless audio compression mostly is realized by\n",
    "using a combination of linear prediction or a\n",
    "transformation followed by entropy coding. The\n",
    "linear predictor attempts to minimize the variance of\n",
    "the difference signal between the predicted sample\n",
    "value and its actual value. The entropy coder\n",
    "(Huffmann, LZW) allocates short codewords to\n",
    "samples with high probability of occurance and\n",
    "longer codewords to samples with lower probability\n",
    "and in this way reduces the average bit consumption. \n",
    "\n",
    "* In the context of signal encoding, entropy coding refeers to assign short codewords to hightly probable (quantization levels) levels and longer code-words to less probable levels, yilding a short average code-word length.\n",
    "\n",
    "* FLAC was [started in 2000](https://en.wikipedia.org/wiki/FLAC#History). In 2003 it becomes as a project in the Xiph.Org Foundation.\n",
    "* Lossless. Typically compress 2:1.\n",
    "* [Supports](https://en.wikipedia.org/wiki/FLAC#Source_encoder) only fixed-point samples, not floating-point. It can handle any PCM bit resolution from 4 to 24 bits per sample, any sampling rate from 1 Hz to 65,535 Hz in 1 Hz increments or from 10 Hz to 655,350 Hz in 10 Hz increments, and any number of channels from 1 to 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding algorithm:\n",
    "1. Split the input in blocks of samples (4096 by default).\n",
    "2. Perform inter-channel decorrelation: mid = (left + right) / 2, side = left - right.\n",
    "1. Intra-channel decorrelation: Decorrelate PCM samples using linear prediction. Available predictors are: Zero, Verbatim, Fixed Linear (fitting a simple polynomial to the signal) and FIR Linear (linear predictive coding (LPC)) ... pero vamos, que ni idea.\n",
    "2. Encode the residuals using Golomb Coding and RLE (silences). Rice coding involves finding a single parameter that matches a signal's distribution, then using that parameter to generate the codes. As the distribution changes, the optimal parameter changes, so FLAC supports a method that allows the parameter to change as needed. The residual can be broken into several contexts or partitions, each with it's own Rice parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lossless audio compression:\n",
    "\n",
    "framming -> (inter-channel dec) -> Intra-channel decorrelation -> Entropy coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Framming: for *editibility* and limit the propagation of errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://xiph.org/flac/format.html#interchannel\n",
    "\n",
    "Prediction\n",
    "\n",
    "FLAC uses four methods for modeling the input signal:\n",
    "\n",
    "    Verbatim. This is essentially a zero-order predictor of the signal. The predicted signal is zero, meaning the residual is the signal itself, and the compression is zero. This is the baseline against which the other predictors are measured. If you feed random data to the encoder, the verbatim predictor will probably be used for every subblock. Since the raw signal is not actually passed through the residual coding stage (it is added to the stream 'verbatim'), the encoding results will not be the same as a zero-order linear predictor.\n",
    "    Constant. This predictor is used whenever the subblock is pure DC (\"digital silence\"), i.e. a constant value throughout. The signal is run-length encoded and added to the stream.\n",
    "    Fixed linear predictor. FLAC uses a class of computationally-efficient fixed linear predictors (for a good description, see audiopak and shorten). FLAC adds a fourth-order predictor to the zero-to-third-order predictors used by Shorten. Since the predictors are fixed, the predictor order is the only parameter that needs to be stored in the compressed stream. The error signal is then passed to the residual coder.\n",
    "    FIR Linear prediction. For more accurate modeling (at a cost of slower encoding), FLAC supports up to 32nd order FIR linear prediction (again, for information on linear prediction, see audiopak and shorten). The reference encoder uses the Levinson-Durbin method for calculating the LPC coefficients from the autocorrelation coefficients, and the coefficients are quantized before computing the residual. Whereas encoders such as Shorten used a fixed quantization for the entire input, FLAC allows the quantized coefficient precision to vary from subframe to subframe. The FLAC reference encoder estimates the optimal precision to use based on the block size and dynamic range of the original signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Residual Coding\n",
    "\n",
    "FLAC currently defines two similar methods for the coding of the error signal from the prediction stage. The error signal is coded using Rice codes in one of two ways: 1) the encoder estimates a single Rice parameter based on the variance of the residual and Rice codes the entire residual using this parameter; 2) the residual is partitioned into several equal-length regions of contiguous samples, and each region is coded with its own Rice parameter based on the region's mean. (Note that the first method is a special case of the second method with one partition, except the Rice parameter is based on the residual variance instead of the mean.)\n",
    "\n",
    "The FLAC format has reserved space for other coding methods. Some possiblities for volunteers would be to explore better context-modeling of the Rice parameter, or Huffman coding. See LOCO-I and pucrunch for descriptions of several universal codes.\n",
    "\n",
    "The samples in the prediction residual are assumed to be uncorrelated and therefore, may be coded indepently.\n",
    "\n",
    "The problem of residual coding is therefore to find an appropiate form for the probability density function (PDF) of the distribution of residuals vaules so that they can be efficiently modelled.\n",
    "\n",
    "The optimal number of low order bits to be transmitted directly is linearly related the the variance of the signal.\n",
    "\n",
    "The Laplacian is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "  p(x) = 1/sqrt{2}\\sigma e^{\\sqrt{ }/\\sigma|x|}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  E(|x|) = \\sigma/\\sqrt{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LPC Linear Predictive Coding\n",
    "\n",
    "\\begin{equation}\n",
    "  \\hat{s}(t) = \\sum_{i=1}^p a_i s(t-i)\n",
    "\\end{equation}\n",
    "\n",
    "$p$ the order of the predictor.\n",
    "\n",
    "\\begin{equation}\n",
    "  e(t) = s(t) \\hat{s}(t)\n",
    "\\end{equation}\n",
    "\n",
    "Durbin's algorithm is used for computing LPC coefficients $a_i$.\n",
    "\n",
    "Autocorrelation coefficients.\n",
    "\n",
    "Approx 50% compression is achieved using 0-th prediction and the best prediction order provides 58%. Hence, for lossless compression it is important not to waste too much compute on the predictor an to perform the residual coding efficiently.\n",
    "\n",
    "In most applications, a FIR predictor is used, and the coefficients\n",
    "of the prediction filter $\\hat{A(z)}$ are determined to \n",
    "minimize the mean-squared prediction error. Without the\n",
    "quantizer and $\\hat{B}(z)=0$, the predictor coefficients can\n",
    "be found for the FIR case by simply solving a set of\n",
    "linear equations.\n",
    "\n",
    "A simple choice of a predictor from a small library of fixed\n",
    "predictors can be also possible.\n",
    "\n",
    "In general, a minimum mean-squared predictor for audio signals\n",
    "will involve fractional coefficients, which must be quantized\n",
    "and coded as part of the lossless representation, and the \n",
    "predictor must be implementes with fixed-point arithmetic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Intra-channel decorrelation\n",
    "\n",
    "To remove redundancy by decorrelation the samples within a frame.\n",
    "\n",
    "Most algorithms remove redundancy by LPC. Each output frame contains the LPC coefficients and the prediction errors.\n",
    "\n",
    "$e[n]$ should be uncorrelated and therefore, have a flat frequency spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance (TO-DO)\n",
    "Download the WAV file [Arnesen: MAGNIFICAT 4. Et misericordia](http://www.lindberg.no/hires/test/2L-106_04_stereo_44k_16b_01.flac) and expand it. Compute the compression ratio. Compare with [gzip](http://www.gzip.org/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
